defaults:
  - base
  - _self_

project: vq_reflow_wavenet_group_fsq
ckpt_path: results/vq_reflow_bf16/checkpoints/step_000248000.ckpt
resume_weights_only: true

# Lightning Trainer
trainer:
  accelerator: gpu
  devices: auto
  precision: 32
  max_steps: 1_000_000
  # max_steps: 100
  val_check_interval: 2000
  gradient_clip_algorithm: norm
  gradient_clip_val: 1.0
  # limit_val_batches: 0.0

  strategy: ddp #_find_unused_parameters_true
  # strategy:
  #   _target_: lightning.pytorch.strategies.DeepSpeedStrategy
  #   stage: 1
  #   overlap_comm: true

  # profiler:
  #   _target_: lightning.pytorch.profilers.PyTorchProfiler
  #   export_to_chrome: true
  #   filename: prof.txt

sample_rate: 44100
hop_length: 512
num_mels: 160
n_fft: 2048
win_length: 2048

# Dataset Configuration
train_dataset:
  _target_: fish_speech.datasets.vqgan.VQGANDataset
  filelist: /***REMOVED***/workspace/diffusion-test/data/HiFi-TTS/vq_train_filelist.txt
  sample_rate: ${sample_rate}
  hop_length: ${hop_length}
  slice_frames: 512

val_dataset:
  _target_: fish_speech.datasets.vqgan.VQGANDataset
  filelist: /***REMOVED***/workspace/diffusion-test/data/HiFi-TTS/vq_val_filelist.txt
  sample_rate: ${sample_rate}
  hop_length: ${hop_length}

data:
  _target_: fish_speech.datasets.vqgan.VQGANDataModule
  train_dataset: ${train_dataset}
  val_dataset: ${val_dataset}
  num_workers: 4
  batch_size: 32
  val_batch_size: 4

# Model Configuration
model:
  _target_: fish_speech.models.vqgan.VQGAN

  sampling_rate: ${sample_rate}
  weight_reflow: 1.0
  weight_vq: 1.0
  weight_aux_mel: 1.0

  encoder:
    _target_: fish_speech.models.vqgan.modules.convnext.ConvNeXtEncoder
    input_channels: ${num_mels}
    depths: [3, 3, 9, 3]
    dims: [128, 256, 384, 512]
  
  quantizer:
    _target_: fish_speech.models.vqgan.modules.fsq.DownsampleFiniteScalarQuantize
    input_dim: 512
    n_codebooks: 1
    n_groups: 8
    levels: [8, 5, 5, 5]
  
  aux_decoder:
    _target_: fish_speech.models.vqgan.modules.convnext.ConvNeXtEncoder
    input_channels: 512
    output_channels: ${num_mels}
    depths: [6]
    dims: [384]

  # reflow:
  #   _target_: fish_speech.models.vqgan.modules.dit.DiT
  #   hidden_size: 768
  #   num_heads: 12
  #   diffusion_num_layers: 12
  #   channels: ${num_mels}
  #   condition_dim: 512

  reflow:
    _target_: fish_speech.models.vqgan.modules.wavenet.WaveNet
    mel_channels: ${num_mels}
    d_encoder: 512
    residual_channels: 512
    residual_layers: 20

  vocoder:
    _target_: fish_speech.models.vqgan.modules.firefly.FireflyBase
    ckpt_path: checkpoints/firefly-gan-base-002000000.ckpt

  mel_transform:
    _target_: fish_speech.models.vqgan.spectrogram.LogMelSpectrogram
    sample_rate: ${sample_rate}
    n_fft: ${n_fft}
    hop_length: ${hop_length}
    win_length: ${win_length}
    n_mels: ${num_mels}

  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 1e-4
    betas: [0.8, 0.99]
    eps: 1e-5
    weight_decay: 0.01

  lr_scheduler:
    _target_: torch.optim.lr_scheduler.LambdaLR
    _partial_: true
    lr_lambda:
      _target_: fish_speech.scheduler.get_cosine_schedule_with_warmup_lr_lambda
      _partial_: true
      num_warmup_steps: 100
      num_training_steps: ${trainer.max_steps}
      final_lr_ratio: 0

callbacks:
  grad_norm_monitor:
    sub_module: 
      - encoder
      - aux_decoder
      - quantizer
      - reflow

  model_checkpoint:
    every_n_train_steps: ${trainer.val_check_interval}

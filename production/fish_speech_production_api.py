import asyncio
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
import torch
import psutil
import GPUtil
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from concurrent.futures import ThreadPoolExecutor
import redis
import json
import time
import os
import subprocess
from typing import Optional
import logging
import tempfile
import base64

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# H100æœ€é©åŒ–è¨­å®š
os.environ.update({
    'TORCH_COMPILE_DEBUG': '0',
    'CUDA_LAUNCH_BLOCKING': '0',
    'PYTORCH_CUDA_ALLOC_CONF': 'backend:native,max_split_size_mb:512,garbage_collection_threshold:0.8,expandable_segments:True'
})

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
REQUEST_COUNT = Counter('fish_speech_requests_total', 'Total requests', ['status'])
REQUEST_LATENCY = Histogram('fish_speech_request_duration_seconds', 'Request latency')
ACTIVE_REQUESTS = Gauge('fish_speech_active_requests', 'Active requests')
GPU_UTILIZATION = Gauge('fish_speech_gpu_utilization_percent', 'GPU utilization')
GPU_MEMORY = Gauge('fish_speech_gpu_memory_used_gb', 'GPU memory used GB')
QUEUE_SIZE = Gauge('fish_speech_queue_size', 'Queue size')

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
app = FastAPI(title="Fish Speech H100 Production API")
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
executor = ThreadPoolExecutor(max_workers=22)  # 20ã‚³ã‚¢ + 2äºˆå‚™
fish_speech_path = '/workspace/fish-speech'

class AudioRequest(BaseModel):
    text: str
    reference_audio: Optional[str] = None
    reference_text: Optional[str] = None
    temperature: float = 0.7
    top_p: float = 0.9
    max_new_tokens: int = 1024
    compile: bool = True

class AudioResponse(BaseModel):
    audio_data: str  # base64 encoded
    processing_time: float
    tokens_generated: int
    status: str

def run_fish_speech_inference(text: str, **kwargs):
    """Fish Speech CLIãƒ™ãƒ¼ã‚¹æ¨è«–"""
    try:
        start_time = time.time()
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(text)
            text_file = f.name
        
        # Fish Speechæ¨è«–ã‚³ãƒãƒ³ãƒ‰æ§‹ç¯‰
        cmd = [
            'python', 
            f'{fish_speech_path}/fish_speech/models/text2semantic/inference.py',
            '--text', text,
            '--checkpoint-path', f'{fish_speech_path}/checkpoints/openaudio-s1-mini',
            '--num-samples', '1',
            '--max-new-tokens', str(kwargs.get('max_new_tokens', 1024)),
            '--temperature', str(kwargs.get('temperature', 0.7)),
            '--top-p', str(kwargs.get('top_p', 0.9))
        ]
        
        # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æœ€é©åŒ–
        if kwargs.get('compile', True):
            cmd.append('--compile')
        
        # H100æœ€é©åŒ–
        if torch.cuda.is_available() and 'H100' in torch.cuda.get_device_name(0):
            cmd.extend(['--half'])  # BF16ä½¿ç”¨
        
        # æ¨è«–å®Ÿè¡Œ
        os.chdir(fish_speech_path)
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=60  # 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        )
        
        if result.returncode != 0:
            logger.error(f"Fish Speechæ¨è«–ã‚¨ãƒ©ãƒ¼: {result.stderr}")
            raise Exception(f"æ¨è«–å¤±æ•—: {result.stderr}")
        
        # ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        codes_file = None
        for i in range(10):  # codes_0.npy ~ codes_9.npy
            candidate = f'codes_{i}.npy'
            if os.path.exists(candidate):
                codes_file = candidate
                break
        
        if not codes_file:
            raise Exception("ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆã«å¤±æ•—")
        
        # éŸ³å£°ç”Ÿæˆï¼ˆVQGANï¼‰
        audio_cmd = [
            'python',
            f'{fish_speech_path}/fish_speech/models/vqgan/inference.py',
            '-i', codes_file,
            '--checkpoint-path', f'{fish_speech_path}/checkpoints/openaudio-s1-mini/firefly-gan-vq-fsq-8x1024-21hz-generator.pth'
        ]
        
        audio_result = subprocess.run(
            audio_cmd,
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if audio_result.returncode != 0:
            logger.error(f"éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {audio_result.stderr}")
            raise Exception(f"éŸ³å£°ç”Ÿæˆå¤±æ•—: {audio_result.stderr}")
        
        # ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        audio_file = 'fake.wav'
        if os.path.exists(audio_file):
            with open(audio_file, 'rb') as f:
                audio_data = base64.b64encode(f.read()).decode('utf-8')
        else:
            raise Exception("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã«å¤±æ•—")
        
        processing_time = time.time() - start_time
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        for temp_file in [text_file, codes_file, audio_file]:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
        
        return {
            'audio_data': audio_data,
            'processing_time': processing_time,
            'tokens_generated': len(text) // 4,  # æ¨å®š
            'status': 'success'
        }
        
    except subprocess.TimeoutExpired:
        logger.error("æ¨è«–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
        raise HTTPException(status_code=504, detail="æ¨è«–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
    except Exception as e:
        logger.error(f"æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚åˆæœŸåŒ–"""
    logger.info("ğŸš€ Fish Speech H100 APIèµ·å‹•ä¸­...")
    
    # ç’°å¢ƒç¢ºèª
    if torch.cuda.is_available():
        logger.info(f"âœ… CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.get_device_name(0)}")
        logger.info(f"ğŸ“Š VRAMå®¹é‡: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB")
    else:
        logger.warning("âš ï¸ CUDAåˆ©ç”¨ä¸å¯")
    
    # Fish Speechãƒ‘ã‚¹ç¢ºèª
    if os.path.exists(fish_speech_path):
        logger.info(f"âœ… Fish Speechãƒ‘ã‚¹ç¢ºèª: {fish_speech_path}")
    else:
        logger.error(f"âŒ Fish Speechãƒ‘ã‚¹ä¸å­˜åœ¨: {fish_speech_path}")
    
    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
    asyncio.create_task(update_metrics())

async def update_metrics():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°"""
    while True:
        try:
            # GPUæƒ…å ±æ›´æ–°
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                GPU_UTILIZATION.set(gpu.load * 100)
                GPU_MEMORY.set(gpu.memoryUsed / 1024)  # GBå¤‰æ›
            
            # ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºæ›´æ–°
            queue_size = redis_client.llen('audio_queue') if redis_client.ping() else 0
            QUEUE_SIZE.set(queue_size)
            
        except Exception as e:
            logger.error(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        
        await asyncio.sleep(1)

@app.post("/generate", response_model=AudioResponse)
async def generate_audio(request: AudioRequest):
    """éŸ³å£°ç”Ÿæˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    ACTIVE_REQUESTS.inc()
    
    try:
        with REQUEST_LATENCY.time():
            # éåŒæœŸå®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                executor,
                run_fish_speech_inference,
                request.text,
                **request.dict(exclude={'text'})
            )
            
        REQUEST_COUNT.labels(status='success').inc()
        return AudioResponse(**result)
        
    except Exception as e:
        REQUEST_COUNT.labels(status='error').inc()
        logger.error(f"éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
    finally:
        ACTIVE_REQUESTS.dec()

@app.get("/metrics")
async def metrics():
    """Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    return generate_latest()

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    try:
        gpu_memory = torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0
        queue_size = redis_client.llen('audio_queue') if redis_client.ping() else 0
        
        return {
            "status": "healthy",
            "gpu_available": torch.cuda.is_available(),
            "gpu_memory_gb": round(gpu_memory, 2),
            "active_requests": ACTIVE_REQUESTS._value._value,
            "queue_size": queue_size,
            "fish_speech_path": fish_speech_path,
            "fish_speech_exists": os.path.exists(fish_speech_path)
        }
    except Exception as e:
        logger.error(f"ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return {"status": "unhealthy", "error": str(e)}

if __name__ == "__main__":
    # H100æœ€é©åŒ–Uvicornè¨­å®š
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        workers=1,  # ã‚·ãƒ³ã‚°ãƒ«ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆGPUå…±æœ‰å›é¿ï¼‰
        loop="uvloop",
        http="httptools",
        access_log=False,  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–
        log_config={
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "default": {
                    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                },
            },
            "handlers": {
                "default": {
                    "formatter": "default",
                    "class": "logging.StreamHandler",
                    "stream": "ext://sys.stdout",
                },
            },
            "root": {
                "level": "INFO",
                "handlers": ["default"],
            },
        }
    )
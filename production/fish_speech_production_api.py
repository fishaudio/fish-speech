import asyncio
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
import torch
import psutil
import GPUtil
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from concurrent.futures import ThreadPoolExecutor
import redis
import json
import time
import os
import subprocess
from typing import Optional
import logging
import tempfile
import base64
from functools import partial

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# H100æœ€é©åŒ–è¨­å®š
os.environ.update({
    'TORCH_COMPILE_DEBUG': '0',
    'CUDA_LAUNCH_BLOCKING': '0',
    'PYTORCH_CUDA_ALLOC_CONF': 'backend:native,max_split_size_mb:512,garbage_collection_threshold:0.8,expandable_segments:True'
})

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
REQUEST_COUNT = Counter('fish_speech_requests_total', 'Total requests', ['status'])
REQUEST_LATENCY = Histogram('fish_speech_request_duration_seconds', 'Request latency')
ACTIVE_REQUESTS = Gauge('fish_speech_active_requests', 'Active requests')
GPU_UTILIZATION = Gauge('fish_speech_gpu_utilization_percent', 'GPU utilization')
GPU_MEMORY = Gauge('fish_speech_gpu_memory_used_gb', 'GPU memory used GB')
QUEUE_SIZE = Gauge('fish_speech_queue_size', 'Queue size')

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
app = FastAPI(title="Fish Speech H100 Production API")
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
executor = ThreadPoolExecutor(max_workers=22)  # 20ã‚³ã‚¢ + 2äºˆå‚™
fish_speech_path = '/workspace/fish-speech'

class AudioRequest(BaseModel):
    text: str
    reference_audio: Optional[str] = None
    reference_text: Optional[str] = None
    temperature: float = 0.7
    top_p: float = 0.9
    max_new_tokens: int = 1024
    compile: bool = True

class AudioResponse(BaseModel):
    audio_data: str  # base64 encoded
    processing_time: float
    tokens_generated: int
    status: str

def run_fish_speech_inference(text: str, **kwargs):
    """Fish Speech CLIãƒ™ãƒ¼ã‚¹æ¨è«–"""
    try:
        start_time = time.time()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä»˜ãï¼‰
        temperature = kwargs.get('temperature', 0.7)
        top_p = kwargs.get('top_p', 0.9)
        max_new_tokens = kwargs.get('max_new_tokens', 1024)
        compile_enabled = kwargs.get('compile', True)
        reference_audio = kwargs.get('reference_audio')
        reference_text = kwargs.get('reference_text')
        
        logger.info(f"ğŸµ éŸ³å£°ç”Ÿæˆé–‹å§‹: ãƒ†ã‚­ã‚¹ãƒˆé•·={len(text)}, compile={compile_enabled}")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
            f.write(text)
            text_file = f.name
        
        # Fish Speechæ¨è«–ã‚³ãƒãƒ³ãƒ‰æ§‹ç¯‰
        cmd = [
            'python', 
            f'{fish_speech_path}/fish_speech/models/text2semantic/inference.py',
            '--text', text,
            '--checkpoint-path', f'{fish_speech_path}/checkpoints/openaudio-s1-mini',
            '--num-samples', '1',
            '--max-new-tokens', str(max_new_tokens),
            '--temperature', str(temperature),
            '--top-p', str(top_p)
        ]
        
        # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æœ€é©åŒ–
        if compile_enabled:
            cmd.append('--compile')
            logger.info("âœ… ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æœ€é©åŒ–æœ‰åŠ¹")
        
        # H100æœ€é©åŒ–
        if torch.cuda.is_available():
            device_name = torch.cuda.get_device_name(0)
            if 'H100' in device_name:
                cmd.extend(['--half'])  # BF16ä½¿ç”¨
                logger.info("ğŸš€ H100æœ€é©åŒ–ï¼ˆBF16ï¼‰æœ‰åŠ¹")
            logger.info(f"ğŸ“Š GPU: {device_name}")
        
        # å‚ç…§éŸ³å£°ãŒã‚ã‚‹å ´åˆã®å‡¦ç†ï¼ˆä»Šå¾Œã®å®Ÿè£…ç”¨ï¼‰
        if reference_audio and reference_text:
            logger.info("ğŸ¯ å‚ç…§éŸ³å£°ãƒ¢ãƒ¼ãƒ‰ï¼ˆç¾åœ¨æœªå®Ÿè£…ï¼‰")
            # TODO: å‚ç…§éŸ³å£°å¯¾å¿œã®å®Ÿè£…
        
        # æ¨è«–å®Ÿè¡Œ
        logger.info(f"ğŸ”„ æ¨è«–å®Ÿè¡Œ: {' '.join(cmd[:3])}...")
        os.chdir(fish_speech_path)
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=60,  # 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            env=dict(os.environ, PYTHONPATH=fish_speech_path)
        )
        
        if result.returncode != 0:
            logger.error(f"âŒ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {result.stderr}")
            raise Exception(f"ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ç”Ÿæˆå¤±æ•—: {result.stderr}")
        
        logger.info("âœ… ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ç”Ÿæˆå®Œäº†")
        
        # ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        codes_file = None
        for i in range(10):  # codes_0.npy ~ codes_9.npy
            candidate = f'codes_{i}.npy'
            if os.path.exists(candidate):
                codes_file = candidate
                logger.info(f"ğŸ“ ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«: {codes_file}")
                break
        
        if not codes_file:
            raise Exception("ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆã«å¤±æ•—")
        
        # éŸ³å£°ç”Ÿæˆï¼ˆVQGANï¼‰
        vocoder_path = f'{fish_speech_path}/checkpoints/openaudio-s1-mini/firefly-gan-vq-fsq-8x1024-21hz-generator.pth'
        if not os.path.exists(vocoder_path):
            # ä»£æ›¿ãƒ‘ã‚¹è©¦è¡Œ
            vocoder_path = f'{fish_speech_path}/checkpoints/openaudio-s1-mini/codec.pth'
        
        audio_cmd = [
            'python',
            f'{fish_speech_path}/fish_speech/models/vqgan/inference.py',
            '-i', codes_file,
            '--checkpoint-path', vocoder_path
        ]
        
        logger.info("ğŸ¼ éŸ³å£°ç”Ÿæˆé–‹å§‹...")
        audio_result = subprocess.run(
            audio_cmd,
            capture_output=True,
            text=True,
            timeout=30,
            env=dict(os.environ, PYTHONPATH=fish_speech_path)
        )
        
        if audio_result.returncode != 0:
            logger.error(f"âŒ éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {audio_result.stderr}")
            raise Exception(f"éŸ³å£°ç”Ÿæˆå¤±æ•—: {audio_result.stderr}")
        
        logger.info("âœ… éŸ³å£°ç”Ÿæˆå®Œäº†")
        
        # ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        audio_file = 'fake.wav'
        if os.path.exists(audio_file):
            with open(audio_file, 'rb') as f:
                audio_data = base64.b64encode(f.read()).decode('utf-8')
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
            file_size = os.path.getsize(audio_file)
            logger.info(f"ğŸµ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ: {file_size} bytes")
        else:
            raise Exception("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã«å¤±æ•—")
        
        processing_time = time.time() - start_time
        tokens_generated = len(text) // 4  # æ¨å®šå€¤
        
        logger.info(f"ğŸ‰ éŸ³å£°ç”ŸæˆæˆåŠŸ: {processing_time:.2f}s, {tokens_generated} tokens")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        cleanup_files = [text_file, codes_file, audio_file]
        for temp_file in cleanup_files:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
        
        return {
            'audio_data': audio_data,
            'processing_time': processing_time,
            'tokens_generated': tokens_generated,
            'status': 'success'
        }
        
    except subprocess.TimeoutExpired:
        logger.error("â° æ¨è«–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
        raise HTTPException(status_code=504, detail="æ¨è«–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
    except Exception as e:
        logger.error(f"ğŸ’¥ æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ä»˜ãã§ãƒ­ã‚°å‡ºåŠ›
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        # ç¢ºå®Ÿã«ãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æˆ»ã™
        try:
            os.chdir('/workspace')
        except:
            pass

@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚åˆæœŸåŒ–"""
    logger.info("ğŸš€ Fish Speech H100 APIèµ·å‹•ä¸­...")
    
    # ç’°å¢ƒç¢ºèª
    if torch.cuda.is_available():
        logger.info(f"âœ… CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.get_device_name(0)}")
        logger.info(f"ğŸ“Š VRAMå®¹é‡: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB")
    else:
        logger.warning("âš ï¸ CUDAåˆ©ç”¨ä¸å¯")
    
    # Fish Speechãƒ‘ã‚¹ç¢ºèª
    if os.path.exists(fish_speech_path):
        logger.info(f"âœ… Fish Speechãƒ‘ã‚¹ç¢ºèª: {fish_speech_path}")
    else:
        logger.error(f"âŒ Fish Speechãƒ‘ã‚¹ä¸å­˜åœ¨: {fish_speech_path}")
    
    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
    asyncio.create_task(update_metrics())

async def update_metrics():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°"""
    while True:
        try:
            # GPUæƒ…å ±æ›´æ–°
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                GPU_UTILIZATION.set(gpu.load * 100)
                GPU_MEMORY.set(gpu.memoryUsed / 1024)  # GBå¤‰æ›
            
            # ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºæ›´æ–°
            queue_size = redis_client.llen('audio_queue') if redis_client.ping() else 0
            QUEUE_SIZE.set(queue_size)
            
        except Exception as e:
            logger.error(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        
        await asyncio.sleep(1)

@app.post("/generate", response_model=AudioResponse)
async def generate_audio(request: AudioRequest):
    """éŸ³å£°ç”Ÿæˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    ACTIVE_REQUESTS.inc()
    
    try:
        with REQUEST_LATENCY.time():
            # å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿æŠ½å‡º
            kwargs = {
                'temperature': request.temperature,
                'top_p': request.top_p,
                'max_new_tokens': request.max_new_tokens,
                'compile': request.compile
            }
            
            # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå€¤ãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ ï¼‰
            if request.reference_audio:
                kwargs['reference_audio'] = request.reference_audio
            if request.reference_text:
                kwargs['reference_text'] = request.reference_text
            
            # éåŒæœŸå®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            func = partial(run_fish_speech_inference, request.text, **kwargs)
            result = await loop.run_in_executor(executor, func)
            
        REQUEST_COUNT.labels(status='success').inc()
        return AudioResponse(**result)
        
    except Exception as e:
        REQUEST_COUNT.labels(status='error').inc()
        logger.error(f"éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
    finally:
        ACTIVE_REQUESTS.dec()

@app.get("/metrics")
async def metrics():
    """Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    return generate_latest()

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    try:
        gpu_memory = torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0
        queue_size = redis_client.llen('audio_queue') if redis_client.ping() else 0
        
        return {
            "status": "healthy",
            "gpu_available": torch.cuda.is_available(),
            "gpu_memory_gb": round(gpu_memory, 2),
            "active_requests": ACTIVE_REQUESTS._value._value,
            "queue_size": queue_size,
            "fish_speech_path": fish_speech_path,
            "fish_speech_exists": os.path.exists(fish_speech_path)
        }
    except Exception as e:
        logger.error(f"ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return {"status": "unhealthy", "error": str(e)}

if __name__ == "__main__":
    # H100æœ€é©åŒ–Uvicornè¨­å®š
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        workers=1,  # ã‚·ãƒ³ã‚°ãƒ«ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆGPUå…±æœ‰å›é¿ï¼‰
        loop="uvloop",
        http="httptools",
        access_log=False,  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–
        log_config={
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "default": {
                    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                },
            },
            "handlers": {
                "default": {
                    "formatter": "default",
                    "class": "logging.StreamHandler",
                    "stream": "ext://sys.stdout",
                },
            },
            "root": {
                "level": "INFO",
                "handlers": ["default"],
            },
        }
    )
#!/usr/bin/env python3
import torch
import sys
import os

def main():
    print('ğŸ”„ Fish Speechç’°å¢ƒç¢ºèª...')
    
    # ãƒ‘ã‚¹è¨­å®š
    fish_speech_path = '/workspace/fish-speech'
    sys.path.append(fish_speech_path)
    os.chdir(fish_speech_path)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
    model_path = './checkpoints/openaudio-s1-mini'
    if os.path.exists(model_path):
        print(f'âœ… ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹å­˜åœ¨: {model_path}')
        try:
            files = os.listdir(model_path)
            print(f'ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {files}')
            
            # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
            important_files = ['config.yaml', 'model.pth', 'codec.pth']
            for file in important_files:
                if file in files:
                    file_size = os.path.getsize(os.path.join(model_path, file)) / 1024**2
                    print(f'   âœ… {file}: {file_size:.1f}MB')
                else:
                    print(f'   âš ï¸ {file}: ä¸å­˜åœ¨')
        except Exception as e:
            print(f'âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}')
    else:
        print(f'âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ä¸å­˜åœ¨: {model_path}')
        print('ğŸ’¡ HuggingFaceã‹ã‚‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦ã§ã™')
        return False
    
    # CUDAç’°å¢ƒç¢ºèª
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        print(f'âœ… CUDAåˆ©ç”¨å¯èƒ½: {device_name}')
        
        device_props = torch.cuda.get_device_properties(0)
        vram_gb = device_props.total_memory / 1024**3
        print(f'ğŸ“Š VRAMå®¹é‡: {vram_gb:.1f}GB')
        print(f'ğŸ“Š SMæ•°: {device_props.multi_processor_count}')
        print(f'ğŸ“Š CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.version.cuda}')
        
        # H100æœ€é©åŒ–ç¢ºèª
        if 'H100' in device_name:
            print('ğŸš€ H100æ¤œå‡º - æœ€é©åŒ–é©ç”¨ä¸­...')
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            torch.backends.cuda.enable_flash_sdp(True)
            print('âœ… TF32 + FlashAttentionæœ‰åŠ¹åŒ–å®Œäº†')
            
            # H100å›ºæœ‰æ©Ÿèƒ½ç¢ºèª
            if hasattr(torch.cuda, 'get_device_capability'):
                capability = torch.cuda.get_device_capability(0)
                print(f'ğŸ“Š Compute Capability: {capability[0]}.{capability[1]}')
        
        # ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆ
        try:
            print('ğŸ§ª GPUãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...')
            initial_memory = torch.cuda.memory_allocated()
            
            # è»½é‡ãƒ†ã‚¹ãƒˆ
            test_tensor = torch.randn(1000, 1000, device='cuda', dtype=torch.float32)
            memory_used = torch.cuda.memory_allocated() - initial_memory
            print(f'ğŸ“Š ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆæˆåŠŸ: {memory_used/1024**2:.1f}MBä½¿ç”¨')
            
            # å¤§å®¹é‡ãƒ†ã‚¹ãƒˆï¼ˆH100ã®å ´åˆï¼‰
            if 'H100' in device_name and vram_gb > 50:
                print('ğŸš€ H100å¤§å®¹é‡ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆ...')
                large_tensor = torch.randn(10000, 10000, device='cuda', dtype=torch.bfloat16)
                large_memory = torch.cuda.memory_allocated() - initial_memory
                print(f'ğŸ“Š å¤§å®¹é‡ãƒ†ã‚¹ãƒˆæˆåŠŸ: {large_memory/1024**3:.2f}GBä½¿ç”¨')
                del large_tensor
            
            del test_tensor
            torch.cuda.empty_cache()
            
            # ãƒ¡ãƒ¢ãƒªæƒ…å ±
            memory_info = torch.cuda.memory_get_info()
            free_memory = memory_info[0] / 1024**3
            total_memory = memory_info[1] / 1024**3
            used_memory = total_memory - free_memory
            print(f'ğŸ“Š ãƒ¡ãƒ¢ãƒªçŠ¶æ³: {used_memory:.1f}GBä½¿ç”¨ / {total_memory:.1f}GBç·å®¹é‡')
            
        except Exception as e:
            print(f'âŒ GPUãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}')
            return False
    else:
        print('âŒ CUDAåˆ©ç”¨ä¸å¯')
        print('ğŸ’¡ NVIDIA GPUã¨CUDAãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ç¢ºèªãŒå¿…è¦ã§ã™')
        return False
    
    # PyTorchæœ€é©åŒ–è¨­å®šç¢ºèª
    print('\nğŸ”§ PyTorchæœ€é©åŒ–è¨­å®šç¢ºèª:')
    print(f'   TF32 MatMul: {torch.backends.cuda.matmul.allow_tf32}')
    print(f'   TF32 cuDNN: {torch.backends.cudnn.allow_tf32}')
    print(f'   cuDNN Benchmark: {torch.backends.cudnn.benchmark}')
    print(f'   cuDNN Deterministic: {torch.backends.cudnn.deterministic}')
    
    # ç’°å¢ƒå¤‰æ•°ç¢ºèª
    print('\nğŸŒ é‡è¦ãªç’°å¢ƒå¤‰æ•°:')
    important_env_vars = [
        'CUDA_VISIBLE_DEVICES',
        'PYTORCH_CUDA_ALLOC_CONF', 
        'TORCH_COMPILE_DEBUG',
        'OMP_NUM_THREADS'
    ]
    
    for var in important_env_vars:
        value = os.environ.get(var, 'æœªè¨­å®š')
        print(f'   {var}: {value}')
    
    print('\nâœ… ãƒ¢ãƒ‡ãƒ«äº‹å‰ç¢ºèªå®Œäº†')
    return True

if __name__ == '__main__':
    try:
        success = main()
        if success:
            print('\nğŸ‰ ç’°å¢ƒç¢ºèªæˆåŠŸ - Fish Speechå®Ÿè¡Œæº–å‚™å®Œäº†')
            sys.exit(0)
        else:
            print('\nâŒ ç’°å¢ƒç¢ºèªå¤±æ•— - è¨­å®šã‚’è¦‹ç›´ã—ã¦ãã ã•ã„')
            sys.exit(1)
    except KeyboardInterrupt:
        print('\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­')
        sys.exit(1)
    except Exception as e:
        print(f'\nğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}')
        import traceback
        traceback.print_exc()
        sys.exit(1)
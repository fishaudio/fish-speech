#!/bin/bash
set -e

# =============================================================================
# Fish Speech H100 æœ¬ç•ªç’°å¢ƒæœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# 100åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¯¾å¿œ - 80GB VRAM + 20vCPU + 251GB RAM
# ç›®æ¨™: 150-300 requests/second, 1:15 real-time factor
# =============================================================================


# =============================================================================
# å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# =============================================================================

echo "ðŸ“¦ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«..."

apt update && apt upgrade -y
# åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæœ€å°æ§‹æˆï¼‰
echo "ðŸ“¦ åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«..."
apt install -y \
    build-essential cmake ca-certificates \
    libsox-dev libasound-dev portaudio19-dev \
    libportaudio2 libportaudiocpp0 \
    ffmpeg git wget curl

# æœ¬ç•ªç’°å¢ƒãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼ˆæŽ¨å¥¨ï¼‰
echo "ðŸš€ æœ¬ç•ªç’°å¢ƒãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«..."
apt install -y \
    htop \
    redis-server \
    supervisor \
    util-linux

# =============================================================================
# CUDA & PyTorchç’°å¢ƒè¨­å®šï¼ˆH100æœ€é©åŒ–ï¼‰
# =============================================================================
echo "âš¡ CUDAç’°å¢ƒè¨­å®šï¼ˆH100ç‰¹åŒ–ï¼‰..."

# åŸºæœ¬CUDAè¨­å®š
export CUDA_VISIBLE_DEVICES=0
export CUDA_DEVICE_ORDER=PCI_BUS_ID

# H100 80GB VRAMæœ€é©åŒ–ãƒ¡ãƒ¢ãƒªç®¡ç†
export PYTORCH_CUDA_ALLOC_CONF="backend:native,max_split_size_mb:512,roundup_power2_divisions:4,garbage_collection_threshold:0.8,expandable_segments:True"

# CUDAæŽ¥ç¶šæ•°æœ€å¤§åŒ–ï¼ˆH100å¯¾å¿œï¼‰
export CUDA_DEVICE_MAX_CONNECTIONS=32
export CUDA_MODULE_LOADING=LAZY
export CUDA_LAUNCH_BLOCKING=0

# H100 Transformer Engineæœ€é©åŒ–
export TORCH_CUDNN_V8_API_ENABLED=1
export TORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1
export TORCH_CUDNN_ALLOW_TF32=1
export NVTE_ALLOW_NONDETERMINISTIC_ALGO=1
export NVTE_FUSED_ATTN=1
export NVTE_FLASH_ATTN=1

# PyTorch 2.x ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æœ€é©åŒ–
export TORCH_COMPILE_DEBUG=0
export TORCHINDUCTOR_CACHE_DIR=/tmp/torch_compile_cache
export TORCHINDUCTOR_FX_GRAPH_CACHE=1
export TORCH_COMPILE_MODE=max-autotune

# =============================================================================
# CPU & ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼ˆ20ã‚³ã‚¢ + 251GB RAMï¼‰
# =============================================================================
echo "ðŸ§  CPUãƒ»ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼ˆ20ã‚³ã‚¢å¯¾å¿œï¼‰..."

# 20ã‚³ã‚¢CPUæœ€é©åŒ–è¨­å®š
export OMP_NUM_THREADS=20
export MKL_NUM_THREADS=20
export OPENBLAS_NUM_THREADS=20
export VECLIB_MAXIMUM_THREADS=20
export TORCH_NUM_THREADS=20
export TORCH_NUM_INTEROP_THREADS=4

# NUMA & CPUã‚¢ãƒ•ã‚£ãƒ‹ãƒ†ã‚£æœ€é©åŒ–
export OMP_PROC_BIND=CLOSE
export OMP_PLACES=cores
export GOMP_CPU_AFFINITY="0-19"

# NCCLè¨­å®šï¼ˆãƒžãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹å¯¾å¿œï¼‰
export NCCL_DEBUG=WARN
export NCCL_IB_DISABLE=0
export NCCL_SOCKET_IFNAME=^docker0,lo

# =============================================================================
# Fish Speechç’°å¢ƒæ§‹ç¯‰
# =============================================================================
echo "ðŸŸ Fish Speechç’°å¢ƒæ§‹ç¯‰..."

# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install --upgrade pip
pip install -e .[stable]
pip install huggingface_hub

# =============================================================================
# ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ & ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
# =============================================================================
echo "ðŸ“¥ ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰..."

huggingface-cli download fishaudio/openaudio-s1-mini --local-dir checkpoints/openaudio-s1-mini

python3 /workspace/fish-speech/production/model_check.py

# =============================================================================
# Redisè¨­å®šï¼ˆã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°ç”¨ï¼‰
# =============================================================================

echo "ðŸ”´ Redisè¨­å®šï¼ˆé«˜æ€§èƒ½ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°ï¼‰..."

mkdir -p /etc/redis /var/lib/redis /var/log/redis
chown redis:redis /var/lib/redis /var/log/redis

cat > /etc/redis/redis.conf << EOF
# åŸºæœ¬è¨­å®š
bind 127.0.0.1
port 6379
timeout 0
tcp-keepalive 300
daemonize yes

# é«˜æ€§èƒ½è¨­å®š
maxmemory 8gb
maxmemory-policy allkeys-lru
save ""
stop-writes-on-bgsave-error no

# AOFç„¡åŠ¹åŒ–ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹é‡è¦–ï¼‰
appendonly no

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æœ€é©åŒ–
tcp-backlog 2048
timeout 300

# ãƒ­ã‚°è¨­å®š
loglevel notice
logfile /var/log/redis/redis-server.log
EOF


echo "ðŸ‘¥ Supervisorè¨­å®š..."

mkdir -p /etc/supervisor/conf.d

cat > /etc/supervisor/conf.d/fish-speech-production.conf << EOF
[program:redis-server]
command=/usr/bin/redis-server /etc/redis/redis.conf
autostart=true
autorestart=true
user=redis
redirect_stderr=true
stdout_logfile=/var/log/supervisor/redis.log

[program:fish-speech-api]
command=/usr/bin/python /workspace/fish-speech/production/fish_speech_production_api.py
directory=/workspace/fish-speech
autostart=true
autorestart=true
user=root
redirect_stderr=true
stdout_logfile=/var/log/supervisor/fish-speech-api.log
environment=CUDA_VISIBLE_DEVICES=0,PYTHONPATH="/workspace/fish-speech"
numprocs=1
process_name=%(program_name)s_%(process_num)02d

[group:fish-speech]
programs=redis-server,fish-speech-api
priority=999
EOF

# =============================================================================
# èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆ & ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
# =============================================================================

pip install \
    huggingface_hub \
    nvidia-ml-py3 \
    uvicorn[standard] \
    fastapi \
    redis \
    celery \
    gunicorn \
    prometheus-client \
    psutil \
    gpustat

echo "ðŸŽ¯ èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ..."

chmod +x /workspace/start_fish_speech_production.sh
./production/start_fish_speech_production.sh
#!/bin/bash
set -e

# =============================================================================
# Fish Speech H100 æœ¬ç•ªç’°å¢ƒæœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# 100åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¯¾å¿œ - 80GB VRAM + 20vCPU + 251GB RAM
# ç›®æ¨™: 150-300 requests/second, 1:15 real-time factor
# =============================================================================


# =============================================================================
# å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# =============================================================================

echo "ğŸ“¦ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«..."

apt update && apt upgrade -y
apt install -y \
    build-essential cmake ca-certificates \
    libsox-dev libasound-dev portaudio19-dev \
    libportaudio2 libportaudiocpp0 \
    ffmpeg git wget curl

# =============================================================================
# CUDA & PyTorchç’°å¢ƒè¨­å®šï¼ˆH100æœ€é©åŒ–ï¼‰
# =============================================================================
echo "âš¡ CUDAç’°å¢ƒè¨­å®šï¼ˆH100ç‰¹åŒ–ï¼‰..."

# åŸºæœ¬CUDAè¨­å®š
export CUDA_VISIBLE_DEVICES=0
export CUDA_DEVICE_ORDER=PCI_BUS_ID

# H100 80GB VRAMæœ€é©åŒ–ãƒ¡ãƒ¢ãƒªç®¡ç†
export PYTORCH_CUDA_ALLOC_CONF="backend:native,max_split_size_mb:512,roundup_power2_divisions:4,garbage_collection_threshold:0.8,expandable_segments:True"

# CUDAæ¥ç¶šæ•°æœ€å¤§åŒ–ï¼ˆH100å¯¾å¿œï¼‰
export CUDA_DEVICE_MAX_CONNECTIONS=32
export CUDA_MODULE_LOADING=LAZY
export CUDA_LAUNCH_BLOCKING=0

# H100 Transformer Engineæœ€é©åŒ–
export TORCH_CUDNN_V8_API_ENABLED=1
export TORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1
export TORCH_CUDNN_ALLOW_TF32=1
export NVTE_ALLOW_NONDETERMINISTIC_ALGO=1
export NVTE_FUSED_ATTN=1
export NVTE_FLASH_ATTN=1

# PyTorch 2.x ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æœ€é©åŒ–
export TORCH_COMPILE_DEBUG=0
export TORCHINDUCTOR_CACHE_DIR=/tmp/torch_compile_cache
export TORCHINDUCTOR_FX_GRAPH_CACHE=1
export TORCH_COMPILE_MODE=max-autotune

# =============================================================================
# CPU & ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼ˆ20ã‚³ã‚¢ + 251GB RAMï¼‰
# =============================================================================
echo "ğŸ§  CPUãƒ»ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼ˆ20ã‚³ã‚¢å¯¾å¿œï¼‰..."

# 20ã‚³ã‚¢CPUæœ€é©åŒ–è¨­å®š
export OMP_NUM_THREADS=20
export MKL_NUM_THREADS=20
export OPENBLAS_NUM_THREADS=20
export VECLIB_MAXIMUM_THREADS=20
export TORCH_NUM_THREADS=20
export TORCH_NUM_INTEROP_THREADS=4

# NUMA & CPUã‚¢ãƒ•ã‚£ãƒ‹ãƒ†ã‚£æœ€é©åŒ–
export OMP_PROC_BIND=CLOSE
export OMP_PLACES=cores
export GOMP_CPU_AFFINITY="0-19"

# NCCLè¨­å®šï¼ˆãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹å¯¾å¿œï¼‰
export NCCL_DEBUG=WARN
export NCCL_IB_DISABLE=0
export NCCL_SOCKET_IFNAME=^docker0,lo

# =============================================================================
# Fish Speechç’°å¢ƒæ§‹ç¯‰
# =============================================================================
echo "ğŸŸ Fish Speechç’°å¢ƒæ§‹ç¯‰..."

# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install --upgrade pip
pip install -e .[stable]
pip install huggingface_hub

# =============================================================================
# ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ & ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
# =============================================================================
echo "ğŸ“¥ ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰..."

huggingface-cli download fishaudio/openaudio-s1-mini --local-dir checkpoints/openaudio-s1-mini

# 251GB RAMæ´»ç”¨ - ãƒ¢ãƒ‡ãƒ«äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
echo "ğŸ’¾ ãƒ¢ãƒ‡ãƒ«äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ï¼ˆ251GB RAMæ´»ç”¨ï¼‰..."
python3 -c "
import torch
import sys
import os
sys.path.append('/workspace/fish-speech')
os.chdir('/workspace/fish-speech')

print('ğŸ”„ Fish Speechç’°å¢ƒç¢ºèª...')

# ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
model_path = './checkpoints/openaudio-s1-mini'
if os.path.exists(model_path):
    print(f'âœ… ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹å­˜åœ¨: {model_path}')
    files = os.listdir(model_path)
    print(f'ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {files}')
else:
    print(f'âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ä¸å­˜åœ¨: {model_path}')

# CUDAç’°å¢ƒç¢ºèª
if torch.cuda.is_available():
    print(f'âœ… CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.get_device_name(0)}')
    print(f'ğŸ“Š VRAMå®¹é‡: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB')
    
    # H100æœ€é©åŒ–ç¢ºèª
    if 'H100' in torch.cuda.get_device_name(0):
        print('ğŸš€ H100æ¤œå‡º - æœ€é©åŒ–é©ç”¨')
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        print('âœ… TF32æœ‰åŠ¹åŒ–å®Œäº†')
    
    # ãƒ¡ãƒ¢ãƒªç¢ºä¿ãƒ†ã‚¹ãƒˆï¼ˆè»½é‡ï¼‰
    test_tensor = torch.randn(1000, 1000, device='cuda')
    print(f'ğŸ“Š ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆæˆåŠŸ: {torch.cuda.memory_allocated()/1024**2:.1f}MBä½¿ç”¨')
    del test_tensor
    torch.cuda.empty_cache()
else:
    print('âŒ CUDAåˆ©ç”¨ä¸å¯')

print('âœ… ãƒ¢ãƒ‡ãƒ«äº‹å‰ç¢ºèªå®Œäº†')
"